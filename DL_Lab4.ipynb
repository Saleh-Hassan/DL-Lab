{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1 Cell segmentation data\n",
    "The class ”Data” in the main.py file will load the training and testing data. The get train image list and label list function will randomly sample an 300×300 image and the according 116 × 116 labels out of the training data images. We will use those samples to train the U-net. Since we will use a batchsize of 1, the function will put 1 image and its labels into the according lists. The get test image list and label list function will output a list with 12 new images (again with 300×300 resolution) and a list with the according labels (again with 116 × 116 resolution). Those images are not present in the training set. Use them for the validation accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        with h5py.File(\"cell_data.h5\", \"r\") as data:\n",
    "            self.train_images = [data[\"/train_image_{}\".format(i)][:] for i in range(28)]\n",
    "            self.train_labels = [data[\"/train_label_{}\".format(i)][:] for i in range(28)]\n",
    "            self.test_images = [data[\"/test_image_{}\".format(i)][:] for i in range(3)]\n",
    "            self.test_labels = [data[\"/test_label_{}\".format(i)][:] for i in range(3)]\n",
    "\n",
    "        self.input_resolution = 300\n",
    "        self.label_resolution = 116\n",
    "\n",
    "        self.offset = (300 - 116) // 2\n",
    "\n",
    "    def get_train_image_list_and_label_list(self):\n",
    "        n = random.randint(0, len(self.train_images) - 1)\n",
    "        x = random.randint(0, (self.train_images[n].shape)[1] - self.input_resolution - 1)\n",
    "        y = random.randint(0, (self.train_images[n].shape)[0] - self.input_resolution - 1)\n",
    "        image = self.train_images[n][y:y + self.input_resolution, x:x + self.input_resolution, :]\n",
    "\n",
    "        x += self.offset\n",
    "        y += self.offset\n",
    "        label = self.train_labels[n][y:y + self.label_resolution, x:x + self.label_resolution]\n",
    "\n",
    "        return [image], [label]\n",
    "\n",
    "    def get_test_image_list_and_label_list(self):\n",
    "        coord_list = [[0, 0], [0, 116], [0, 232],\n",
    "                      [116, 0], [116, 116], [116, 232],\n",
    "                      [219, 0], [219, 116], [219, 232]]\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "\n",
    "        for image_id in range(3):\n",
    "            for y, x in coord_list:\n",
    "                image = self.test_images[image_id][y:y + self.input_resolution, x:x + self.input_resolution, :]\n",
    "                image_list.append(image)\n",
    "                x += self.offset\n",
    "                y += self.offset\n",
    "                label = self.test_labels[image_id][y:y + self.label_resolution, x:x + self.label_resolution]\n",
    "                label_list.append(label)\n",
    "\n",
    "        return image_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2 U-net implementation\n",
    "Implement the U-net with the architecture on slide 11 of the segmentation slides with the following reductions: Decrease the input resolution to 300×300 and use only half of the number of filters for each layer. Those reductions are necessary to fit the network on the available gpu memory of the computer pool.\n",
    "Do not use any padding in all layers (⇔ use ’valid’ padding). If everything is implemented correctly the output resolution should be 116 × 116.\n",
    "In tensorflow you can get the shape of each tensor using:\n",
    "tensor.get shape().as list(). Use it to crop the filters to the according dimension during the ”Reusing Features” part of the network. You can use tf.concat to combine the cropped features with the new features.\n",
    "Each convolution layer uses [3 × 3] filters with stride 1 and the output con- volution uses a [1 × 1] filter with stride 1. Each max pooling or transposed convolution layer uses a pooling or filter size of [2 × 2] and a stride of 2. We use a ReLu nonlinearity after each convolutional layer except for output layer.\n",
    "For the weight tensor initialization use the tensorflow default initializer: tf.glorot uniform initializer(). It will be automatically selected if the initializer is set to None. For the bias vector use the 0 initialization: tf.zeros initializer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class U_Net_Layers:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "    def conv2d_transpose(self, x, output_filter_size):\n",
    "        return tf.layers.conv2d_transpose(inputs=x, filters=output_filter_size,\n",
    "                                          kernel_size=2,\n",
    "                                          strides=2,\n",
    "                                          padding='VALID')\n",
    "    \n",
    "    def weight_variable(self, weight_shape):\n",
    "        value = tf.truncated_normal(weight_shape, stddev=0.1)\n",
    "        return tf.Variable(value)\n",
    "\n",
    "    def bias_variable(self, bias_shape):\n",
    "        value = tf.constant(0.1, shape=bais_shape)\n",
    "        return tf.Variable(value)\n",
    "\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1], padding='VALID')\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    def create_convolution_layer(self, input_image, num_filters,filter_size=3):\n",
    "        num_input_channels = int(input_image.get_shape()[3])\n",
    "        weight_shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "        weights = self.weight_variable(shape=weight_shape)\n",
    "        biases = self.bias_variable([num_filters])\n",
    "        layer = self.conv2d(input_image, weights)\n",
    "        layer = tf.nn.relu(layer + biases)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def do_pooling(self, layer):\n",
    "        return self.max_pool_2x2(layer)\n",
    "\n",
    "    def create_transposed_convolution(self, input_image):\n",
    "        output_filters = int(int(input_image.get_shape()[3])/2)\n",
    "        return self.conv2d_transpose(input_image, output_filters)\n",
    "\n",
    "    def crop_tensor(self, tensor, height, width):\n",
    "        return tf.image.resize_image_with_crop_or_pad(tensor, height, width)\n",
    "\n",
    "    def crop_and_merge_layers(self, layer_to_be_cropped, second_layer):\n",
    "        size = int(second_layer.get_shape()[1])\n",
    "        cropped_tensor_1 = nn.crop_tensor(layer_to_be_cropped, size, size)\n",
    "        return tf.concat([second_layer, cropped_tensor_1], 3)\n",
    "\n",
    "class Utilities:\n",
    "\n",
    "    def plot_image(self, im):\n",
    "        figure = plt.figure()\n",
    "        ax = plt.Axes(figure, [0., 0., 1., 1.])\n",
    "        figure.add_axes(ax)\n",
    "        ax.imshow(im, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    def get_accuracy(self, prediction, labels):\n",
    "        prediction = np.argmax(prediction, axis=3)\n",
    "        correct_pix = np.sum(prediction == labels)\n",
    "        inccorect_pix = np.sum(prediction != labels)\n",
    "        total_pixels = correct_pix + inccorect_pix\n",
    "        accuracy = correct_pix / (total_pixels + inccorect_pix)\n",
    "        return accuracy\n",
    "\n",
    "    def compute_validation_accuracy(self, sess, validation_xs, validation_ys):\n",
    "        y_pre = sess.run(output_conv, feed_dict={x_image: validation_xs, y_label: validation_ys})\n",
    "        return self.get_accuracy(y_pre, validation_ys)\n",
    "\n",
    "    def compute_training_accuracy(self, nn_output, y_labels):\n",
    "        return self.get_accuracy(nn_output, y_labels)\n",
    "\n",
    "    def write_to_file(self, file_name, value):\n",
    "        file_name += '.txt'\n",
    "        with open(file_name, 'a') as the_file:\n",
    "            the_file.write(value + '\\n')\n",
    "\n",
    "\n",
    "def create_unet_architecture(U_Net_Layers):\n",
    "    global x_image, y_label, output_conv\n",
    "\n",
    "    x_image = tf.placeholder(tf.float32, [None, 300, 300, 1])\n",
    "    y_label = tf.placeholder(tf.int32, [None, 116, 116])\n",
    "\n",
    "    conv_layer1 = nn.create_convolution_layer(x_image, num_filters=32)\n",
    "    conv_layer2 = nn.create_convolution_layer(conv_layer1, num_filters=32)\n",
    "    pooling_layer1 = nn.do_pooling(conv_layer2)\n",
    "\n",
    "    conv_layer3 = nn.create_convolution_layer(pooling_layer1, num_filters=64)\n",
    "    conv_layer4 = nn.create_convolution_layer(conv_layer3, num_filters=64)\n",
    "    pooling_layer2 = nn.do_pooling(conv_layer4)\n",
    "\n",
    "    conv_layer5 = nn.create_convolution_layer(pooling_layer2, num_filters=128)\n",
    "    conv_layer6 = nn.create_convolution_layer(conv_layer5, num_filters=128)\n",
    "    pooling_layer3 = nn.do_pooling(conv_layer6)\n",
    "\n",
    "    conv_layer7 = nn.create_convolution_layer(pooling_layer3, num_filters=256)\n",
    "    conv_layer8 = nn.create_convolution_layer(conv_layer7, num_filters=256)\n",
    "    pooling_layer4 = nn.do_pooling(conv_layer8)\n",
    "\n",
    "    conv_layer9 = nn.create_convolution_layer(pooling_layer4, num_filters=512)\n",
    "    conv_layer10 = nn.create_convolution_layer(conv_layer9, num_filters=512)\n",
    "    up_conv_layer1 = nn.create_transposed_convolution(conv_layer10)\n",
    "\n",
    "    merged_1 = nn.crop_and_merge_layers(layer_to_be_cropped=conv_layer8, second_layer=up_conv_layer1)\n",
    "\n",
    "    conv_layer11 = nn.create_convolution_layer(merged_1, num_filters=512)\n",
    "    conv_layer12 = nn.create_convolution_layer(conv_layer11, num_filters=256)\n",
    "    up_conv_layer2 = nn.create_transposed_convolution(conv_layer12) \n",
    "\n",
    "    merged_2 = nn.crop_and_merge_layers(layer_to_be_cropped=conv_layer6, second_layer=up_conv_layer2)\n",
    "\n",
    "    conv_layer13 = nn.create_convolution_layer(merged_2, num_filters=256)\n",
    "    conv_layer14 = nn.create_convolution_layer(conv_layer13, num_filters=128)\n",
    "    up_conv_layer3 = nn.create_transposed_convolution(conv_layer14) \n",
    "\n",
    "    merged_3 = nn.crop_and_merge_layers(layer_to_be_cropped=conv_layer4, second_layer=up_conv_layer3)\n",
    "\n",
    "    conv_layer15 = nn.create_convolution_layer(merged_3, num_filters=128)\n",
    "    conv_layer16 = nn.create_convolution_layer(conv_layer15, num_filters=64)\n",
    "    up_conv_layer4 = nn.create_transposed_convolution(conv_layer16)\n",
    "\n",
    "    merged_4 = nn.crop_and_merge_layers(layer_to_be_cropped=conv_layer2, second_layer=up_conv_layer4)\n",
    "\n",
    "    conv_layer17 = nn.create_convolution_layer(merged_4, num_filters=64)\n",
    "    conv_layer18 = nn.create_convolution_layer(conv_layer17, num_filters=32)\n",
    "    output_conv = nn.create_convolution_layer(conv_layer18, num_filters=2, filter_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Training of the Autoencoder\n",
    "Implement the U-net with the architecture on slide 11 of the segmentation slides with the following reductions: Decrease the input resolution to 300×300 and use only half of the number of filters for each layer. Those reductions are necessary to fit the network on the available gpu memory of the computer pool.\n",
    "Do not use any padding in all layers (⇔ use ’valid’ padding). If everything is implemented correctly the output resolution should be 116 × 116.\n",
    "In tensorflow you can get the shape of each tensor using:\n",
    "tensor.get shape().as list(). Use it to crop the filters to the according dimension during the ”Reusing Features” part of the network. You can use tf.concat to combine the cropped features with the new features.\n",
    "Each convolution layer uses [3 × 3] filters with stride 1 and the output con- volution uses a [1 × 1] filter with stride 1. Each max pooling or transposed convolution layer uses a pooling or filter size of [2 × 2] and a stride of 2. We use a ReLu nonlinearity after each convolutional layer except for output layer.\n",
    "For the weight tensor initialization use the tensorflow default initializer: tf.glorot uniform initializer(). It will be automatically selected if the initializer is set to None. For the bias vector use the 0 initialization: tf.zeros initializer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Step: ', 0)\n",
      "('Validation accuracy: ', 0)\n",
      "('Step: ', 5)\n",
      "('Validation accuracy: ', 0)"
     ]
    }
   ],
   "source": [
    "def perform_tf_operations():\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    y_plot_validation = []\n",
    "    y_plot_train = []\n",
    "    x_plot = []\n",
    "\n",
    "    for i in range(10):\n",
    "        batch_xs, batch_ys = data.get_train_image_list_and_label_list()\n",
    "        output, _ = sess.run([output_conv, train_step], feed_dict={x_image: batch_xs, y_label: batch_ys})\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(\"Step: \", i)\n",
    "            training_accuracy = utils.compute_training_accuracy(output, batch_ys)\n",
    "            valid_images, valid_labels = data.get_test_image_list_and_label_list()\n",
    "            validation_accuracy = utils.compute_validation_accuracy(sess, valid_images, valid_labels)\n",
    "            print(\"Validation accuracy: \", validation_accuracy)\n",
    "\n",
    "            utils.write_to_file('epochs', str(i))\n",
    "            x_plot.append(i)\n",
    "\n",
    "            utils.write_to_file('validation_accuracy', str(validation_accuracy))\n",
    "            y_plot_validation.append(validation_accuracy)\n",
    "\n",
    "            utils.write_to_file('training_Accuracy', str(training_accuracy))\n",
    "            y_plot_train.append(training_accuracy)\n",
    "\n",
    "    plt.plot(x_plot, y_plot_validation, label='validation')\n",
    "    plt.plot(x_plot, y_plot_train, label='training')\n",
    "\n",
    "\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    display_images(sess)\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "def display_images(sess):\n",
    "    test_im, _ = data.get_test_image_list_and_label_list()\n",
    "\n",
    "    for i in range(2):\n",
    "        image = test_im[i]\n",
    "        im = np.array([[p[0] for p in l] for l in image])\n",
    "        utils.plot_image(im)\n",
    "        test_out = sess.run(output_conv, feed_dict={x_image: test_im})\n",
    "        test_prediction = np.argmax(test_out, axis=3)\n",
    "        utils.plot_image(test_prediction[i])\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = Data()\n",
    "    nn = Layers()\n",
    "    utils = Utilities()\n",
    "\n",
    "    create_unet_architecture(nn)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_label, logits=output_conv)\n",
    "    total_loss = tf.reduce_mean(loss)\n",
    "    train_step = tf.train.AdamOptimizer(0.0001, 0.95, 0.99).minimize(total_loss)\n",
    "\n",
    "    perform_tf_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-186bf876b8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-186bf876b8be>\u001b[0m in \u001b[0;36mextract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saleh/venv/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saleh/venv/lib/python2.7/site-packages/matplotlib/backend_bases.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saleh/venv/lib/python2.7/site-packages/matplotlib/backends/backend_tkagg.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0mTk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m\"\"\"Run the main loop of Tcl.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0m_default_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0mgetint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract():\n",
    "    steps = open('epochs.txt', 'r').read()\n",
    "    training_data = open('training_Accuracy.txt', 'r').read()\n",
    "    validation_data = open('validation_accuracy.txt', 'r').read()\n",
    "\n",
    "\n",
    "    steps = steps.split('\\n')\n",
    "    training_data = training_data.split('\\n')\n",
    "    validation_data = validation_data.split('\\n')\n",
    "\n",
    "    del steps[-1]\n",
    "    del training_data[-1]\n",
    "    del validation_data[-1]\n",
    "\n",
    "    plt.plot(steps, validation_data, label='validation')\n",
    "    plt.plot(steps, training_data, label='training')\n",
    "\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "extract()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
